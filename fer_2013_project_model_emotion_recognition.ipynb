{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "fname = '/content/drive/MyDrive/fer2013_face_recognition_dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(fname, \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "kxk7ZQ-UZ9SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOrbFwWeaJOx",
        "outputId": "22060e2e-2d7b-4d69-f30c-5f7d3cd8cb8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-05 15:04:28--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py.1’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-05 15:04:28 (70.3 MB/s) - ‘helper_functions.py.1’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_text = {0:'angry', 1:'disgust', 2:'fear', 3:'happy',4:'neutral', 5:'sad,', 6: 'surprise'}\n",
        "class_names=['angry','disgust','fear','happy','neutral','sad','surprise']\n"
      ],
      "metadata": {
        "id": "DJ4Gj2DHiDNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import create_tensorboard_callback,compare_historys,make_confusion_matrix"
      ],
      "metadata": {
        "id": "glh3khPIbns4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir=\"/content/train\"\n",
        "test_dir='/content/test'"
      ],
      "metadata": {
        "id": "QTgI7Jjkb52h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import tensorflow as tf\n",
        " train_data = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
        "                                           batch_size=32,\n",
        "                                           image_size=(48,48),\n",
        "                                           class_names=class_names,\n",
        "                                           shuffle=True,\n",
        "                                           seed=42)\n",
        "\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
        "                                           batch_size=32,\n",
        "                                           image_size=(48,48),\n",
        "                                           class_names=class_names,\n",
        "                                           shuffle=True,\n",
        "                                           seed=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THPrRhRs3EiT",
        "outputId": "46e94c89-9e9c-4139-b2f6-2d0d03a827a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 28709 files belonging to 7 classes.\n",
            "Found 7178 files belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from helper_functions import walk_through_dir\n",
        "walk_through_dir(train_dir)\n",
        "walk_through_dir(test_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cFCjLbFdc0X",
        "outputId": "b33f5328-cc22-4645-eca7-df6fce7d5644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 7 directories and 0 images in '/content/train'.\n",
            "There are 0 directories and 3171 images in '/content/train/surprise'.\n",
            "There are 0 directories and 7215 images in '/content/train/happy'.\n",
            "There are 0 directories and 4965 images in '/content/train/neutral'.\n",
            "There are 0 directories and 3995 images in '/content/train/angry'.\n",
            "There are 0 directories and 4097 images in '/content/train/fear'.\n",
            "There are 0 directories and 4830 images in '/content/train/sad'.\n",
            "There are 0 directories and 436 images in '/content/train/disgust'.\n",
            "There are 7 directories and 0 images in '/content/test'.\n",
            "There are 0 directories and 831 images in '/content/test/surprise'.\n",
            "There are 0 directories and 1774 images in '/content/test/happy'.\n",
            "There are 0 directories and 1233 images in '/content/test/neutral'.\n",
            "There are 0 directories and 958 images in '/content/test/angry'.\n",
            "There are 0 directories and 1024 images in '/content/test/fear'.\n",
            "There are 0 directories and 1247 images in '/content/test/sad'.\n",
            "There are 0 directories and 111 images in '/content/test/disgust'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "metadata": {
        "id": "vddzxE7wfMJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_angry='/content/train/angry'"
      ],
      "metadata": {
        "id": "oP0mOXvDlGmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img=mpimg.imread('/content/train/surprise/Training_10275630.jpg')\n",
        "plt.imshow(img)\n",
        "plt.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "c38iPBKWmlfI",
        "outputId": "89b7cb42-b40e-420a-a268-5abb26ba8028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de4wd133fv7+Z+969+14ul1yKIilZD1exjcjyK0Zjuy5kx42dxE3zQKsGBgy0DeAgCWy5LYoGaAu7QOOkaJpAqA0rsBE7TlLIdpMmih8J/IgsSrblSLIp6kHxsct9L3fv+86c/rGXKn+P5V6R4uVK8/sABHdmz5w5c2bOnf197+9BIQQ4jvPKJ7reA3AcZzD4YnecjOCL3XEygi92x8kIvtgdJyP4YnecjHBVi52I7iaiHxHRSSK696UalOM4Lz10pd+zE1EM4ASAdwI4A+BhAL8YQnhip2PyhaFQKo2Ljvo4mRyi8RHVLfKO0pK+rnK5zbYb7bxqk7sgOjemJ7fR5E2SRLWhOFb7QprqznSrXVsQ9fEZHYmJNbqV46acHjNSfaC8Dor0eORzRWTcaLHPmh91XGxcu3qGjXPJ+TDGjG5X71MDsuZ+gL4qu5yqkWyinTbMVZW7itPeBeBkCOEZACCizwF4L4AdF3upNI4ff+Ov8p3yJhhQl19hd0g/lOtH+aVcuE3fuDtue55tP/78rGoz+ZUS245benbH/+JJtp1c2FJt4tERtS+t18UOawXu/oFAhYI4mZ4PKoo2xoOcrG/wbsYm9HCaLbUvrdXYdlQZ0se1O3w8Bf3BKq9DzQ8AyvH7Go1U9bk6/FzmfJSK/JihsmqD88t6n/iwoWJRt+nrQ/wKMF7Eu72cv738hR1/dzV/xh8EcPqS7TO9fY7j7EGuuUBHRB8kouNEdLzTqe1+gOM414SrWexnARy6ZHuut48RQrgvhHBnCOHOfF7/uec4zmC4Gpv9YQA3E9ERbC/yXwDwSy+2k6jN7Z0Qaxs+zQu7qavtlkiYbeUpbf+9e/oHbPuH3zqi2hQ3+HiGntP2eD/QUEXti4Q9nmxc0G2ETWjZaFTmugKVSqoNhPiWNpq6TcRtW8s+p7x+ROJJbttbx0ntwTo/tfhxUVXb4+nmJu+2rW3mIPqWdj4AtG/lFubaq/ScldbG1b6Rv+L6DMqGrW+Jj7txpcL4bvrAZYZyxYs9hNAlol8F8JcAYgCfCiE8fqX9OY5zbbmaNztCCH8O4M9forE4jnMNcQ86x8kIV/VmvxJImCrSRifju2f5XXe3rL9H7Qhz79D4umrz0IWj/NyGfZOrc1s3anVUm/TIHB/fuSXdkeX8cWCGb4vvubdPyI+zvq+XNnKyqL8fJuv8so2wx+V34wAQLEcTeY8M3wDlaGQ4o8i+k3V9z+RxqeHTEI3ym9991Zxq0x7h3/OXVvWY81vaOQr7Jvl2w9AnpINOH74jSPr4br4PR6S+HIouDmv3MzqO80rAF7vjZARf7I6TEXyxO05GGLhAJ6N2oo5wvijozx8p2sUtLW50RnibN089o9rc//03su2Rc1rMKC1wl97NW7SjReECF5ZKSzrIwwwgmR7lO+66Q7VpV3lwSH2f7rtb3j2iLV/nOyuLbdUmbvDriNe0I1IwnGpogQuSZDiahCIftyW8ot5gm8maFuiCcLwJqRbRkpVVtp0WDqs20nmrtKTnozGjHW2aN3IHovJJQ4yVIpkVUSfbWAKqnCNT6BP7+hH6LnbXd0vHcV7W+GJ3nIzgi91xMsJAbXZKUuTXeNBCMsRtO8vRRe5KSvozqr2fO4S8rapzaPwhvYFt7/+WDkTpjoggEyNgQdq63QM66QMZttTqq7nzR3tEX+zWDfx8admwyara3lSsc9ufkoJqklb47R96dli1aU3q6y+uiEAYy0QVfk+xMWQS5nds+KuMPsvva+VbJ1WbZG2NbRfOaWelM+/hDk37/047EJEx1c1J/nwWl/UcRRtc5wn9ODRZtvZL8eq9jD+Pv9kdJyP4YnecjOCL3XEygi92x8kIg3WqIUJa4qeMW8lltwGgO8xFkm5Ff0aVR7lI8qX116k2uVNcfGtP6Oi52gw/18Rj2tEjWueRV41bZlSb1Vu1IFY7KMS3ghZp0gq//lxVC0nFEt/XbOhzJUXRd0nPa7HCVbP0rM4U053Wylp3VKbb1qpQvGmkpRaEmM9Hrq77aY3y+1GdvFW1mfrGPNtOz8yrNgf/iveTVHXGm8K6EfUX8+MaB3VqtUrNyAKkOuLX2peIZzkiSWQm3ctkzfE3u+NkBF/sjpMRfLE7TkYYrM0eAqImd0iRTivSpgd0NtlcTdu6bzn0LNs+2xhTbUae5tvNcX2unMiKkxZ1m7M/d4htN6e1bZXTMSUqA24wZj+qcxusmzOyy4p0P3IbAEjY6KGpbehWwu3W3JBhIyZ9eDlZ5xe3yHK8iTq8o7ilz5VUeN/rN+t+AF7ZZ+obuoUMTEqndPbf2qylffAx5Rr6Wisy8KWfCjF9ZKS15kwdJ8/tNrvjOL7YHScj+GJ3nIzgi91xMsJgo94CQB0hHOW5cBTVjfCoihBOIj3szS4Xmx46oUs7jYxy8SIyMgcnIjFM6w4d5SQjuIbOalGkq/UfBJl5xNBSpCiTq+tMNYkQDa1a9EEIe2QJbeI4eQwARJt6rqO2mEerrLlMutI2HG+MKDdJZYFvj5zSji8yw0v7oJFd6IzIZpPbvczYdju+3Ry38o8L8bPThzOMlc1GYpWIEucKORfoHMcR+GJ3nIzgi91xMsLgs8vKck+iTBB1tUNCUubDbI1qB5HH5g/wfgzTpSNiGHIN3aY5yQ8cP6EN++HHuMdMvLKp2qQVnalUBl809+lgjPoUv7bWhGXY832JUUW4NSmy9g4bhrVwtCms689+yx4nsS/Rl4E8j0tCcV3bn6NP8wCS3Jq+IdEyz0LTXTiv2uT280CkzTfo7LIFoauUzul7lpS1I1ZtJhZtDMcfcV/jZUN3kgErfdjjJjLDTb6PY3r4m91xMoIvdsfJCL7YHScj+GJ3nIww8Kg3JcB1Dc8WQXOCO5YMndMCSKvAnS3aK9oZprzMRREpxgFAQWSXjgwHCWoIxw6jrjnOLahdkahHXjYykVQP8giuxm37VZvmBL9ttVn9md2p8msLDS3klM/xfXJ+tg/Uu1REm6EhTv6AZ/OhR5/U/eSEc5A1j0PcOyk3q+cjneTCWnleC33psEgR3tTnym9pNTIe43O0cZOekFQ4tsSGw4wso2WlKFdYa0NmuOmnnx7+ZnecjOCL3XEywq6LnYg+RUSLRPT3l+ybIKIHieip3v/aGdlxnD1FPzb7pwH8DwB/eMm+ewF8JYTwMSK6t7f9kb7OKO0QaXMYWT5aI/wzqbSqmqDxyCTbHj+tbRnp/NEt6zatV3NHj6ijnWNKi7wjsmzNm25Uu+INbsemq2uqTdjgokH5cT0fdCu367cOaq8WaUeHvL7WgvAr6Qxp43v0OW3H5rb4PSys6OyqdPJ5fn7pVAJd6jk+dEC1kQEjVlbW1n7uLdUe1Y919SSf13RURyrFNSu7LJ/bZFZH7ySijFauoIOXVL99ZKohw8lGHdd/xebd3+whhL8FIJfXewHc3/v5fgDv6/+UjuNcD67UZp8JIVxMzr0AQCdOdxxnT3HVAl0IIcD8gmYbIvogER0nouPtrpGF0XGcgXCli/08Ec0CQO//xZ0ahhDuCyHcGUK4s5AzMjo4jjMQrtSp5osA7gHwsd7/D/R1FJF2LmhxB5lglO7JtbiVsH6TFqSmfsBFo6HT+q+I5/8xL2+UVIzyS2s8K07toGqCma9yZSsZ1yWBajdYqWpG2Wa3NKeaVJ/nAlBuQzuI5Db5nBUu6BTIW0f5tZXO6Vvd4sNB1RA1K2f0PEbrIqTNEJvCET5x6bARGtfm9yw1IrhySxfUPjWehI+7Nqv7Ka7z+5Hf0EJb1NZOLJUlLlCOHNeCbSDh5NXPK7SfVNKWv5lMTCOu/XJONv189fZHAL4N4BYiOkNEH8D2In8nET0F4B/1th3H2cPs+mYPIfziDr96x0s8FsdxriHuQec4GWGwgTBpChJleGq3TbPt/EGdLURmAl27XdslE38sHDsM55yJH3IjaOl1+rMuGeU2WnHVyAQinCaef5cudTz5uJHh5nlu/67dpoN1zr+eO5rMPKxPX9/PbfTKolHm+of81tYPGAEcBb6v/KjuJ9o0yhFLe9OwExd+YoKPcVnfj6HTXI9oj2ntoTM6xbZL3/yhatM9yh04N27V1zF+QmTElcFMAEg4PQFAeoTf28KmkYG3K1PpGu9QOUeWzS7bWK9i6WQkn3PPLus4ji92x8kIvtgdJyP4YnecjDBYgS5JgLUNtqu4MsK2T71LO6OUVrjokJa1AJOWuJAWG2WkimtSfNOOHvnNgtjWgsxzP8Mj7ArrqomZ4aY5xR0yZGkhAAhCD9w4ZqSkltF7FS3KtPm0ojOu56ywxE9W36cH1B2aVPukYGplqmmJw8oruo0sf1Va0g5Ey6/hAlnxFp0mujMs3lkjWnxrV7moWigYZa3K+nkoLXJBOW7oiLZuRZQwa2mhMbfFn0dZBg0AgkizbjoriX27u+ZcMq4X0dZxnJcxvtgdJyP4YnecjOCL3XEywmAFujgGjXLlKN7iAkhlXkeQbdwicxcbfUsHJaNmnExXlBrZgyrneUdbc1oCaRzmAlDx+7qjLSPyKi0IodE4v9x34ahuI2uth1hPSKcq6rMb9dETof21R3Sb5pS+jn5q5pHQnzaO6vfK1kHekVUPLogn9OzbRlSb4TPiXq/riZWpzYrjWvgsnjbSf4v0VcVV/Vw1p3lfIW+kkhbppqltFbXf3TNRpaB+Ea9rf7M7Tkbwxe44GcEXu+NkhIHXZw8Rt0uaszzya3heOxvU/yE3CpNEf0Z1y9xOCzM6okxGJ7XGLccXvj1667Jqg7/jjYLxkZmWtP0r7djWmGFrj3KbMK4bNdOFw0532IhoK/N+IqOfWAS0tXXAIXJG2kC5z6oPHwm/lsKGES0m/Z4MLaafNqu3i9rrS/palZPRiJHNRneto+M62tYuicizpKw1g6Qklpplj4usM9TQjmEy0xOkI85l8De742QEX+yOkxF8sTtORvDF7jgZYeACHYma5JuHuJix8SrrGOGM0jYcPUTkk0wvDACFNRF5lGplqTvGVbSfPHBStflymYd0NQ1lp9OHaGYRNUWtb63RqCizeL9Rj3yDD8qq9SajqqSACACVBSPddp4ft3nYiLobFWKoIf7lt0hsG0MUY7IEwxbPgIVYZ4lWDkStqn6Ghq00zFaNdEHU5KJdMFJiyyi3tGhE3XWk85gW+qSzWKD+39f+ZnecjOCL3XEygi92x8kIA7XZk6E81u7az/atvkY4f7SMgI0WH2ZxSBuyrVGeHaS0qp0fagdFwIJx9dTin39nm9rY7IzvbnuHnFFXvfPiP1vjhuE0IXY1u7pfGfgSGYEwJLMQWzb7kp5HabNvHdS2pczCIx14AB2s05qw0jQLu35j9/mIDJt985i8OG1XTzT1gSTqwdOaLkcVqjygJ17Wz6esBy+dywBd5z020qFLRxuSTj6pFSW2jb/ZHScj+GJ3nIzgi91xMoIvdsfJCIMV6HJAbb+oVSXENqppsScIp5pj0zoS7ek5nnI46up0vvUZ3k9xVY+x8Ga+82enH1FtHopv4scsG04+1d2j3qT4BGgBynKqkdls0o7hxCE/xme1+NQ5zx1v8pt6PK0x/YhIp57hs1oUqol71pzRYlNOONVY8yHnLCkbkYIiTXauZoy5wtuE2Kjhl2iFkhpi3or6uaKWcZNkG+EMExmZlFpVfj9ya9pZSo4nmRpl2yod9aXn3HWUjuO8IvDF7jgZwRe742SEgdrsIQ80ZrjNVapyG6RV17ZUeYR7ZDyzrEsS5Tf5dnNC2y4yOCVKdJs3zz7Htt9TWVJtPhLxfqwssbHhHCTtz/KibiMdQro64Q4m3jbPtkeN+kvnnuXZdOKctkelqS+DVwBg6bW675Fn+bYVeDI0z/vK1/R7RQbHRFbCVTEka65LkyKT0ZIxacLxKCkZDjzTU2pfyPFJoi0jEkcE0IRJI+pHQC19P2QgTDKkI6yiLb4WohrfltluWNtdR+U4zisCX+yOkxF8sTtORth1sRPRISL6GhE9QUSPE9GHevsniOhBInqq9//4tR+u4zhXSj8CXRfAb4QQHiWiKoBHiOhBAP8SwFdCCB8jonsB3AvgI5frKETaKeLIxBrbPrGis8fMjHL17dST+1WbfSKjiqwhDgBjT3NRZPU2fflHy1yQq0TaieKmY7xM0KnvzKk2ZIhNpVXhRKLLiKNTldtacLmhKuZsbVqf6zy/tsKPtGg1ItI7L9+lRaPcqJHO+BS/R4Ut7SDSGBflltaM0k6y/rjhDyLTdFsC3WS1xrbPTulnSEYBtmb1DUr26/dVtCbS50TG+1GIeDCcbLrj3PmluU+Lb7m6eIZLWqwOsZwQOfdXIdCFEOZDCI/2ft4E8CSAgwDeC+D+XrP7Abxvt74cx7l+vCibnYhuBPA6AA8BmAkhXPwOaAHAzA7HfJCIjhPR8WSrZjVxHGcA9L3YiWgYwJ8C+LUQAovgDyEE7PD3QwjhvhDCnSGEO+NhXaHVcZzB0JdTDRHlsb3QPxtC+LPe7vNENBtCmCeiWQCLu3YUdMaUrjDKchvaTklS8Zk0ou2t+owIIqjrz56hR3iWkdqMttE2RRrSbza1PTpZ4n+hzBvZUww/F5QXeV+NffqzVmZ4GX1K97PyhVm2nbxBO3GQMONnv7mp2mwe4R++k8f13M/8pXYqCqPc/m8c0mWUw5QIeDLiTsZPctFia78RwCJ2NYy/H//F4b9j25+hN6g25x7jOs/EMR0FFUiXcSaZXbZgiAZtfh1kBNl0qvw4WUIa0DZ7e0TPR26Y6xHxutAUdjbZ+1LjCcAnATwZQvjtS371RQD39H6+B8ADu/XlOM71o583+1sA/HMAPyCi7/X2/VsAHwPwx0T0AQCnAPz8tRmi4zgvBbsu9hDCN2B+KQIAeMdLOxzHca4V7kHnOBlhsOWf8inCHI9QanX5EEKsFYaFVSEAbRhlcWQWGMNhRX60WVFWn33i9Wz7uSM6wu6pVR4dZTnQjD5vlE0Ss50YZaNKK6JGtyG4NA5wgWzkeT2AfE2IREZa4k6Z/8E2/dCaahM6xkQuCNFOZAkCdCRcqn2TFKUNa86EoDutr/WfDJ1g2w+NHlVtlje4qNlN+nvPhSEuiNGGrlGl2hg13FVEm3Hvu2VR+qul5yMZ4RMZG85KO+FvdsfJCL7YHScj+GJ3nIwwUJs9jlOMVnmmj/PCHk8L2gaJRWaYwrrxGSV25Q2nmmSYG0qWXT/8N7xMz9drt6g2hSoPdBja0ucqrurON2/g9lZeVxJSmLZdZffy1LJsk8xuCgBT/4fbuqFh1GgqGQPYzz120oK+HzKApbhhZIYRNmmho9u0RWnlAwe0M8xXG4fZ9quGzqs2j53jfV94bEK1Ga/qAJaoye9jagTLUJvPtSz1BAD1Ga4zNSf1F1xDCyK7zwX9DG3dwB1/xs8KbWbn5LL+ZnecrOCL3XEygi92x8kIvtgdJyMMtvxTO8bqWR6hNfIjPgRLkGp2uOBR1fqLIl83HBJK/FyldZ2ZpS4irypPG2WkbuQqyLBRrj3qGqLdBd4wqBpN2vmkbZSRkupXecUYgKjTnQzp68iLkGOq6mw2aVVnfakf5qJqu7q7QEdG3fBYOprkdT9qHg0vo8fqh9j2z409rNp8ZvKdbHvmYe340qnqaLW4wR9I+QwBQGGFP0etSR09J+ux57VvDkjcxrimBbriGhf6VN13mcnmEvzN7jgZwRe742QEX+yOkxF8sTtORhioQJe/ABz6v2LfJg+P2jykhaTKAhc3LLEn1+DqRn5Ti2+FVR5x1x3SglRxlfe9dYNRj63BhZyC4UFnpbLO1fgYW0e0ILR1lAtHMiU0AKQFUft8SvdTWhK1zYpG3bA7uLhj1Wzraq1JtZPCEgDUD/Axdkb0GCvn+IFG5iqkHX4dZ1dGVZtHc1ygO1LUqbRkCuqh53Sarsacfh5yi9zNMT2kPeg641zEzG8Y6bdn+H3M1fQzUxAec7KOGwBURIRjc5aP2QU6x3F8sTtOVvDF7jgZYbBRb40uhh9fZvs6M9xBI1/TBqB02oiM6KjCpnBYMWxmma2lMaGtxOF5bhOFnJ6idWE2prE+l3QYAYBcjfdde7VuExe41lD5nh6jjCBLCsb5W7xNbsXQMNZ3j4yz9JEgnF+ihnZQqR3k2WuO/YzOiV37Ns8ek1vSRUS6Jf58JGtaezgp8kt/uvUm1Wb4jMgA1NHzUVzSNnJ3mp/fmo/6Pq4zlVb0/aie4XZ8e1Q/V9QW8y/LSgE6bXX/iWr8ze44WcEXu+NkBF/sjpMRfLE7TkYYbCrpEEBNLlTktvh23NFONbkGVyGKG1pckbXEiivaQySIGlzlVd1PZ5i3GTqv2zT2iUg9I5UWGQJdEEJe9btabBp/igsw+U0tGm3N8eNkhBkAdIakI5IWjTaO8H4KhqOH1bd0oqFE37PDD6yw7WfXb1Zt9qXcsYXWdJ4umuMCWWlei1ZRh+8739Eppw5d4Pcxregxd4eNZ6/O70e8ru9H43bueRRineq8cl4850YNwbgunHESQzDd5CJmblNELhoC4kX8ze44GcEXu+NkBF/sjpMRBmuzxzHSce643xkTkRaGyVFe4U4bnSFttzUmROmchra/igvcRizP11UbmWWkNa6nqLKwuyeDzEwCACRSPh/86xXVJj35HNuu3/0a1aa+n19rW8eGoDnH7T+q6znLz3D7r9PQtmbpKa0rDM3z6xg+p3UNnF1gmzP3Pa2axFO8tFYY1imYZUmk0rIVBCV2kL5npUV+rVFDZ4FJp3RWnjTh89aa0PMhNQxZVgvQz4N8FrbbCGelutYHQkvY/uv8GbYco17ob8ffOI7zisIXu+NkBF/sjpMRfLE7TkYYqEAXCAgFkbFDONVYGV4a01w4yrWM2mbLXCSKm0bRdFE3uzE7ppoMPbPBx1PQ6le3yEUbK/Io5PTnaNwSEUumAMOdgcoPfEe1kTISGZF5UUWkGO4aNcNHeGRaqEulC0hbhnOSsU+SEL+PcjwAANEGsRYR0zxvkzPK0XXFhFgZd/LnRO15w2ElKep7HYTYl9/U81gRWYGsZ1gKzwUjm00q0lTHRtYZGuX3DBdETurUEEt7+JvdcTKCL3bHyQi7LnYiKhHRd4jo+0T0OBH9Vm//ESJ6iIhOEtHniUh/se04zp6hH5u9BeDtIYQtIsoD+AYR/QWAXwfwiRDC54joDwB8AMDvX64j6qaIl3mwQyjyz4gLrxE2CbRNPP7osmrTPsADJiIjECUZ50ED0h4EgLTM9YHKKR2c0R7mtr6VXbVbMbKMiNNFP1rTbSJxnGGDKXu8o+3ItC4chgx7uLvA62iRkYHWss9Vu8SwE0VpK0szQBA31sjMIrPwWHNd2BR1zReNRtJGT3WbqG2UqBI15JOyVSJKZDY2HGZkdt38mhYf5NOYjmidI1oTNnpeOEJJHeTSY3f8TY+wzcUz5Hv/AoC3A/iT3v77Abxvt74cx7l+9GWzE1FMRN8DsAjgQQBPA1gPIVz8uD4D4OC1GaLjOC8FfS32EEISQngtgDkAdwG4td8TENEHieg4ER1vJ9oX3XGcwfCi1PgQwjqArwF4E4Axohe+hJwDcHaHY+4LIdwZQrizEBvftTqOMxB2FeiIaBpAJ4SwTkRlAO8E8HFsL/r3A/gcgHsAPLDr2SJSgpx0LClu7i6SYEULW7lhLhpRS4tG6QhvM/SsLpIdb/DoqLSi6x/FQsixnGpkiSYACE2+Lz12SLXBdx/n24bgIgU5skr+RPzWpk0ttEVDXLAMbR0JJtsAWmyznHrSJr+v8ciIaiOvLRjXKtOGl5e10Lc1K0TVc8ZfkEIMtITH8jn9PNSOcMG4uKydYbpT4plu6AdCliOTDjQAQEJEJClgAgjFXQS5ywh0/ajxswDuJ6IY238J/HEI4ctE9ASAzxHRfwLwXQCf7KMvx3GuE7su9hDCYwBeZ+x/Btv2u+M4LwPcg85xMsJAA2E6wzksvnWa7Rt5jttA1We03ZTmReBJWWcUUZlHutpmj+siE8iaLtsb8mJKjI/DnMgM2hrVjhblRSPwRDj6NA9owbL8JNcIpO0LAPG+Kd5mUtvD0YpwBlpbV22kM0xUNuozW4iMKqGhA2ikHU+jeozS/kyN7K7S8cnKUiQ1k0hmaQUQSsKutmzbptYsclt8jvJrRrBQgY8pKeuHJmryfqxAKUm3qpdnTmoP8l54yWbHcXyxO05G8MXuOBnBF7vjZITBppImXaapsY+LNMUl7RARSTGlo4UUWVZKCW0AqCbELiMSTPZjtSmdFyJNMFIQFy1HF+FEYqSbjqa5+EaG+JVO8Ywqzdlh1aZzM29TfcpwajnxHN825gxWOSExpmC0iYa4+JjsH1dtqM1FzKRsOZqIIdYM4VWWUto5m/ILhLKO8FNReNB11AtreozF51fZduPYpGojy4FZZZoS4WhjPR9bR0U5rCXhHGQc88KvdvyN4zivKHyxO05G8MXuOBlhoDZ7rpFi4glhN0s7ycjy0R3mdn1sBF6EmrD1qzqAQ9njho0qbTmqGQ4jZe6gUVrUbepzRoSfsCWToravkn3c1u6O7NPd5PlndK5mOPC0eZvuuOGI9GO8jHJkOJVEq9rxCEFciOH4Q4dm2XZiZdsVASNJ0ShRVefnKi5qTaczzp2Bmge1hlH6Fg/KpFk9r2Rk/Im6fIxp0VgyY/xZa43pNl3hDJSra2GhtMCDsLpVo9SUdKoRz8JVZapxHOeVgS92x8kIvtgdJyP4YnecjDBQgY7aCQqnRU1yITgkE4bzh8B0iBCOHkqMA4DLRAS9gBDtwtqGakIdLgAlZV3X3MxoIoRGMwW1EGW6RuritkhLXFnQHQV5qUaaZCk2WeJTVNLXljvPO49Jz2tzjt/H/LpRRipvpNsWlBfEfe70Xk0AAA4+SURBVK3rfmLhjNKc0GLkxi//A7a9/3/revFBllaCTi/djzNM5by+9419XNRtTljXLkRlw8lHZrypzfJ+EyM9+kX8ze44GcEXu+NkBF/sjpMRBhsIY2SXDcJBJdrSDioFETAhM6UAAAxHG4kKjiloe1TZ+iWdvSWcXWDbzZtuUW2GntZlo5RzkAzggA68sIIh2kN837CRSbc1wcdtZUbJbQknmnhne+9SUuGw1Dkyrdp0hrlNWjAqXUmHEKtkV3SBPw/pkJHtd5Pb8XFbtymtC/vXyC6bDunryNf5s2fdj86ouK8NfR0jJ/jzsHlM6wPSRldZlQEkJT6vE199lm3nNncup+1vdsfJCL7YHScj+GJ3nIzgi91xMsJgBbpuV5VuokmRwWRJON0ASI/Nse3IKK+Dhoi8knWrAVCDixcyDS8AQEY+GY4N6RaPTips6GipYERwFVb5GFtTWkiS9cgtuhXeZuNmHWE3/oSIVjOuo3YDdw4qGY5AVmri2k3cYcYSrYKsStTSEXWhJIUtPY8kylaZb6dF/swMNwzxTdY6NzIQxUZqcUq5GEkNPUdJeYxtS5EVAIqn+b0vL2vHsNVb+PMwfmJnse0FKsKByHqmL/5q994cx3kl4IvdcTKCL3bHyQi+2B0nIwxUoAtJinSDiyDU4oIHGV5tMhKssGUIF/K4riGayXNZY5S1xy2hr8C9/oqntKiYTOnovdx5Xm+tPbFftYm0M5yiepY3unBYi01rt3MPreppPWfFNS6aUVd7bNXmtIjYLfF3xNB5Q3wToh219P1IR7m4lNvQ3pMQNePJqCGfyntmiJF0ap4fI0RWAIhHtFebirIz+i6s8HG3jX42XsfTYI08YdTeIz7XiZGOvLjChT4VlWnUOLyIv9kdJyP4YnecjOCL3XEywmAz1cQRIpENJFnhpXNyB3gKYgAonBGOOEbK37CPl9wJK9omopJwZLAcEIRNmE5P6Dai1rlZakpG6gGqlJKV9URGOlmRcdKWq57Wp2pM8jYbN2onjnxdjMfwVZKplAFg9CRP55waDkRxXdjadZ1uGuDPQrRplLrax52urEw1EPZ397nnVZNomDsQWbXoQ84oB7bFrzUYKcqjZW43L79vTLWRtId1OazKIr/X+Qtan0gqou79oRneoKE1pov4m91xMoIvdsfJCH0vdiKKiei7RPTl3vYRInqIiE4S0eeJqLBbH47jXD9ezJv9QwCevGT74wA+EUK4CcAagA+8lANzHOelpS+BjojmAPwUgP8M4NeJiAC8HcAv9ZrcD+A/Avj9y3aUBoS6SA2c44KCrD0OACRrkBlpotMi7ycynC8wJhxdrDbCOSfa2NLnkuPb1A4aWFzW+4a4uGPVLWuLNMiWg4isLWelsi4tc6eWNGe4EAnHl/wFPa/xir5+SWe/vmfxCS6SpXNaeI23+PmSMV2jjRLhJGLUMku3+BjlMwUAQaYaHzVSlhtpw9OD3Bnm9N1aWDv0P3/AtiNDQ2zM8evIbWkxsDnJ95UXVBPE4h7JGn7hMqnF+n2z/w6AD+P/P+eTANZDCBcl5zMADvbZl+M414FdFzsRvQfAYgjhkSs5ARF9kIiOE9HxdrC+fnEcZxD082f8WwD8NBG9G0AJwAiA3wUwRkS53tt9DsBZ6+AQwn0A7gOA0XjK+CbXcZxBsOtiDyF8FMBHAYCIfhLAb4YQfpmIvgDg/QA+B+AeAA/00ReCcIgJHW6DBKMEUWeaZxkpndKOJtK2I2mfA0AqjrPqs0uHGd0LUpE9xar1DVkvHkCyzANmaNwIlpF27JC2P6MG1xris4Y+IB2GDFs3dIRmIbP9YPueqa7HuI0ef/sHqg1NT7Ht1Cj1FAnHI5lJCAAggpdS4YTVGyTvRzpPAUg3eQBWaBrZbOr6ni29/1Vsu3arPo5EevTDf7ao2my+mjt9NUf1vA7P8/lYuUNnIKq+nwf0LF7g97X94Z3fp1fzPftHsC3WncS2Df/Jq+jLcZxrzItylw0hfB3A13s/PwPgrpd+SI7jXAvcg85xMoIvdsfJCINNJT1cRvdOXie7Nc4FqJHjWtRPi6IGlyEatae4w0rrZu3oMfLgk2xbpbEGQKJmHE3pqLdIiEbJ6XOqTWhrBxXp7BFtaUGI2sLJqKxFvGSIC0K5FSPdtRAapai4vXP3tDiU117Q3TP8HsVjOsorneb7LOcgmVVFZV0BkGyImnnBEGfFGK25j6o8wo4MwTJ35LDat+9hfv7RZ3Xt9/RG7jDUnNFthk/wa6tUjIxMw/w66gf0GFtb/Dn/0uv/gG3/bMUQa3v4m91xMoIvdsfJCL7YHScjDNRmnz28jH//yU+zff/97DvYduNXufMBAPWR1N2n7di1W7i9M/mEdhBRNrphx1KVB2NY5Y8wKtqc1TZ7ZNR1l1lpg1UfXjgd5dZ09pbOpHC2uHFGtYlXuG2XM4KHIK/Nykxq2drCGSkMaRtVQh3ddzgzL3YYDjwio4ycQ8DIFGtkAJJONGRkqkmHjdrvCzxLUqmlryMR2Y8rzxoBNRU+bkq09rBxbPco8c4T/Nn/LwfuZtvznS/teKy/2R0nI/hid5yM4IvdcTKCL3bHyQgDFegKlOBQjjsp3DDEBZBHjx5RxzXHeMTU1Ld0VFFjHxek5ke0aHT4OSGKGAJZ7XZekqm0pAWyWJRxiio6OskqY5XWeF+xVUd8QqRXXtU1w/PiMlLDQSMdE3XF2zoSTNanl1FoOxFy/DgrvTMJEY/WLqg2SY0La1TUY5QCXbK2ptrE49o5SqKi3iwxcFH3HUa4GEvz+tnLr3Bh7cIbtXNOaYnPUWdE37OtOe5E06lqES/az4Xn803+vHSCji584dgdf+M4zisKX+yOkxF8sTtORhiozX6qNYF/ffIX2L4fn+BZSNeP6iFVT3NHhpO/ojPDTH2P2zflZaOMsGFHK8THX1owMqyI4IwgM6ACQGKURBJBNTLjKQB0R3jQT75hBNQsLPF+Y6NsUUVoFkbJLMhxG/2oNgAgM7WOG+WOhB6RLOsMM9JGDy1t+yciqEUGKgHajrdsf3lcZAQ4Wdl8sML7To7pvKrxM9yp6uzbdDejJ7iuU1zT9njuDu6Mk5zUzmP5Ar+PN1X5s/BIZGRM7uFvdsfJCL7YHScj+GJ3nIzgi91xMsJABbp9hU38qxu+zvb95nf+Kds+/O4z6rjwX7kg99q3nlRtHjt2gPfz8zq9cfr6O9h2d1gLdpWnuJBERomofpLfW+IbpkVEn5FuWkZDdWa1SFOQ4tfikmpDm7wkkkzhDWhhMbJSMBuRgfEwd9gJ64bDzLpwPDJEs2BEfulGfLZDV1+HjDBMm0ZKbLljRDtCRRf0/cAMT4m9/FpdomrowDG2fdt/01GQz/zKIba98SrjKdric5Q/rMuKtc7w89cO8WOSsPP729/sjpMRfLE7Tkbwxe44GWGgNnuRurglzwMJRke4XXL6Ye20QG/l243fu1m1mewK2+7Nr1FtcvPcjmzu0845RZnxVGZBgbYbTXvUsC1VmaKqtv9yG9zerN9glIia5Psio2xREKWcrMAcSvhnveUcZNnxMnurGVRSFqWELeccmSk2Mpx6ZAZcq2SzVXpbIo6jrtYLrJLR8ZoIRDL8bjbn+DLauPGQahMJ36ixJ/R7tjXBtYf3/7O/UW0+c5Yvhh+t82e4mey8pP3N7jgZwRe742QEX+yOkxF8sTtORiBLXLlmJyNaAnAKwBSAnevU7E1ejmMGXp7j9jFfOYdDCNPWLwa62F84KdHxEMKdAz/xVfByHDPw8hy3j/na4H/GO05G8MXuOBnhei32+67Tea+Gl+OYgZfnuH3M14DrYrM7jjN4/M94x8kIA1/sRHQ3Ef2IiE4S0b2DPn8/ENGniGiRiP7+kn0TRPQgET3V+3/3ygQDhIgOEdHXiOgJInqciD7U279nx01EJSL6DhF9vzfm3+rtP0JED/Wekc8T0e7lTQcMEcVE9F0i+nJve8+PeaCLnYhiAL8H4F0Abgfwi0R0+yDH0CefBnC32HcvgK+EEG4G8JXe9l6iC+A3Qgi3A3gjgH/Tm9u9PO4WgLeHEF4D4LUA7iaiNwL4OIBPhBBuArAG4APXcYw78SEAT16yvefHPOg3+10AToYQngkhtAF8DsB7BzyGXQkh/C0Amfv4vQDu7/18P4D3DXRQuxBCmA8hPNr7eRPbD+JB7OFxh20uptTJ9/4FAG8H8Ce9/XtqzABARHMAfgrA/+ptE/b4mIHBL/aDAE5fsn2mt+/lwEwIYb738wKAmes5mMtBRDcCeB2Ah7DHx937c/h7ABYBPAjgaQDrIYSLMcJ78Rn5HQAfBnAxTnYSe3/MLtBdCWH7K4w9+TUGEQ0D+FMAvxZCYMnh9uK4QwhJCOG1AOaw/Zffrdd5SJeFiN4DYDGE8Mj1HsuLZaDJKwCcBXBpZP9cb9/LgfNENBtCmCeiWWy/ifYURJTH9kL/bAjhz3q79/y4ASCEsE5EXwPwJgBjRJTrvSn32jPyFgA/TUTvBlACMALgd7G3xwxg8G/2hwHc3FMuCwB+AcAXBzyGK+WLAO7p/XwPgAeu41gUPbvxkwCeDCH89iW/2rPjJqJpIhrr/VwG8E5saw1fA/D+XrM9NeYQwkdDCHMhhBux/fx+NYTwy9jDY36BEMJA/wF4N4AT2LbN/t2gz9/nGP8IwDyADrbtrw9g2y77CoCnAPw1gInrPU4x5p/A9p/ojwH4Xu/fu/fyuAH8GIDv9sb89wD+Q2//UQDfAXASwBcAFK/3WHcY/08C+PLLZczuQec4GcEFOsfJCL7YHScj+GJ3nIzgi91xMoIvdsfJCL7YHScj+GJ3nIzgi91xMsL/A0jBcVwQaXe7AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape\n",
        "img.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pUtwvOHvd7L",
        "outputId": "660a7eae-eca2-45e7-8cec-58957a5dcaa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "img_size=48\n",
        "def preprocess_image(image):\n",
        "  return tf.cast(img,tf.float32)"
      ],
      "metadata": {
        "id": "DGZhSwnYv4Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_image=preprocess_image(img)"
      ],
      "metadata": {
        "id": "_tf-sCUPw2Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeHoLE75xxY1",
        "outputId": "55157c41-fa95-4818-bd7f-6c6a3111e49f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(48, 48), dtype=float32, numpy=\n",
              "array([[ 50.,  51.,  51., ...,  16.,  15.,  15.],\n",
              "       [ 50.,  46.,  49., ...,  18.,  17.,  17.],\n",
              "       [ 49.,  49.,  47., ...,  20.,  19.,  18.],\n",
              "       ...,\n",
              "       [168., 190.,  91., ..., 115., 149., 186.],\n",
              "       [210., 141., 166., ..., 146., 121., 182.],\n",
              "       [224., 156., 112., ..., 192., 119., 163.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "checkpoint_path = \"model_checkpoints/cp.ckpt\" \n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      monitor=\"val_accuracy\", \n",
        "                                                      save_best_only=True, \n",
        "                                                      save_weights_only=True, \n",
        "                                                      verbose=0)"
      ],
      "metadata": {
        "id": "bIrnkftiyMR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "base_model=tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "input_shape=(48,48,3)\n",
        "\n",
        "\n",
        "inputs=layers.Input(input_shape,name='input_layer')\n",
        "x=base_model(inputs,training=False)\n",
        "x=layers.GlobalAveragePooling2D(name='pooling_layer')(x)\n",
        "x=layers.Dense(len(class_names))(x)\n",
        "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x) \n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "HnXAsCIzyYmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThdRpv9lyvII",
        "outputId": "2a5b04d1-5a84-46ec-d6ee-6c81081ff068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 48, 48, 3)]       0         \n",
            "                                                                 \n",
            " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
            "                                                                 \n",
            " pooling_layer (GlobalAverag  (None, 1280)             0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 7)                 8967      \n",
            "                                                                 \n",
            " softmax_float32 (Activation  (None, 7)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,058,538\n",
            "Trainable params: 4,016,515\n",
            "Non-trainable params: 42,023\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "  print(f'{layer.name} {layer.trainable} {layer.dtype}  {layer.dtype_policy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYtc--zm0hW1",
        "outputId": "2e5183c3-9b51-4f29-c748-660228bc4f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_layer True float32  <Policy \"float32\">\n",
            "efficientnetb0 True float32  <Policy \"float32\">\n",
            "pooling_layer True float32  <Policy \"float32\">\n",
            "dense True float32  <Policy \"float32\">\n",
            "softmax_float32 True float32  <Policy \"float32\">\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8oPJ2MY40BQ",
        "outputId": "87d190e7-317b-4075-81d1-5a9b2e9251f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 48, 48, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_fer2013 =                          model.fit(train_data, \n",
        "                                                     epochs=3,\n",
        "                                                     steps_per_epoch=len(train_data),\n",
        "                                                     validation_data=test_data,\n",
        "                                                     validation_steps=int(0.15 * len(test_data)),\n",
        "                                                     callbacks=[create_tensorboard_callback(\"training_logs\", \n",
        "                                                                                            \"efficientnetb0_101_classes_all_data_feature_extract\"),\n",
        "                                                                model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuGFJsfs0zBP",
        "outputId": "5c822e0c-b635-4782-f409-f9e631fc0f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: training_logs/efficientnetb0_101_classes_all_data_feature_extract/20221005-150438\n",
            "Epoch 1/3\n",
            "898/898 [==============================] - 476s 518ms/step - loss: 1.6844 - accuracy: 0.3207 - val_loss: 1.4985 - val_accuracy: 0.4025\n",
            "Epoch 2/3\n",
            "898/898 [==============================] - 425s 474ms/step - loss: 1.3972 - accuracy: 0.4403 - val_loss: 1.4347 - val_accuracy: 0.4309\n",
            "Epoch 3/3\n",
            "898/898 [==============================] - 430s 479ms/step - loss: 1.3078 - accuracy: 0.4876 - val_loss: 1.3705 - val_accuracy: 0.4915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "history_fer2013 =                          model.fit(train_data, \n",
        "                                                     epochs=5,\n",
        "                                                     steps_per_epoch=len(train_data),\n",
        "                                                     validation_data=test_data,\n",
        "                                                     validation_steps=int(0.15 * len(test_data)),\n",
        "                                                     callbacks=[create_tensorboard_callback(\"training_logs\", \n",
        "                                                                                            \"efficientnetb0_101_classes_all_data_feature_extract\"),\n",
        "                                                                model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBQN70ep1PP0",
        "outputId": "fa2bfb20-cbed-4a45-9542-dda2fbfb7e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: training_logs/efficientnetb0_101_classes_all_data_feature_extract/20221005-171109\n",
            "Epoch 1/5\n",
            "898/898 [==============================] - 443s 483ms/step - loss: 1.1987 - accuracy: 0.5407 - val_loss: 1.2790 - val_accuracy: 0.5095\n",
            "Epoch 2/5\n",
            "898/898 [==============================] - 426s 474ms/step - loss: 1.1243 - accuracy: 0.5719 - val_loss: 1.2805 - val_accuracy: 0.5152\n",
            "Epoch 3/5\n",
            "898/898 [==============================] - 424s 472ms/step - loss: 1.0646 - accuracy: 0.5986 - val_loss: 1.2880 - val_accuracy: 0.5312\n",
            "Epoch 4/5\n",
            "898/898 [==============================] - 426s 474ms/step - loss: 1.0423 - accuracy: 0.6095 - val_loss: 1.2745 - val_accuracy: 0.5398\n",
            "Epoch 5/5\n",
            "898/898 [==============================] - 422s 470ms/step - loss: 0.9997 - accuracy: 0.6251 - val_loss: 1.2706 - val_accuracy: 0.5275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "uBjaa8Oy5tCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgk2YyMP6bEH",
        "outputId": "2029b3bc-6a42-42a1-f304-61e50f65fdaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48, 48, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "model_2 = Sequential()\n",
        "\n",
        "model_2.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "\n",
        "model_2.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_2.add(Dropout(0.1))\n",
        "\n",
        "model_2.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_2.add(Dropout(0.1))\n",
        "\n",
        "model_2.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
        "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_2.add(Dropout(0.1))\n",
        "\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dense(512, activation='relu'))\n",
        "model_2.add(Dropout(0.2))\n",
        "\n",
        "model_2.add(Dense(7, activation='softmax'))\n",
        "\n",
        "model_2.compile(optimizer = 'adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "print(model_2.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B5vGhcEoE36",
        "outputId": "5704f908-222f-4b0f-d7d3-49e4d4a4152a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 46, 46, 32)        896       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 44, 44, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 22, 22, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 22, 22, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 20, 20, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 10, 10, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 10, 10, 128)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               2097664   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 7)                 3591      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,489,671\n",
            "Trainable params: 2,489,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzATPjON6tw6",
        "outputId": "35afb608-f7b3-4f1a-eae7-311ec1952322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 48, 48, 3)]       0         \n",
            "                                                                 \n",
            " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
            "                                                                 \n",
            " pooling_layer (GlobalAverag  (None, 1280)             0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 7)                 8967      \n",
            "                                                                 \n",
            " softmax_float32 (Activation  (None, 7)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,058,538\n",
            "Trainable params: 4,016,515\n",
            "Non-trainable params: 42,023\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "history_fer2013_02 =                      model_2.fit(train_data, \n",
        "                                                     epochs=5,\n",
        "                                                     steps_per_epoch=len(train_data),\n",
        "                                                     validation_data=test_data,\n",
        "                                                     validation_steps=int(0.15 * len(test_data)),\n",
        "                                                     callbacks=[create_tensorboard_callback(\"training_logs\", \n",
        "                                                                                            \"efficientnetb0_101_classes_all_data_feature_extract\"),\n",
        "                                                                model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mlSjHy-55VX",
        "outputId": "1de9d8b1-eca6-4e59-817e-06d40b0cf78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: training_logs/efficientnetb0_101_classes_all_data_feature_extract/20221005-175105\n",
            "Epoch 1/5\n",
            "898/898 [==============================] - 451s 502ms/step - loss: 1.6286 - accuracy: 0.3567 - val_loss: 1.5158 - val_accuracy: 0.4167\n",
            "Epoch 2/5\n",
            "898/898 [==============================] - 443s 494ms/step - loss: 1.4607 - accuracy: 0.4402 - val_loss: 1.3756 - val_accuracy: 0.4706\n",
            "Epoch 3/5\n",
            "898/898 [==============================] - 444s 495ms/step - loss: 1.3455 - accuracy: 0.4842 - val_loss: 1.2800 - val_accuracy: 0.5189\n",
            "Epoch 4/5\n",
            "898/898 [==============================] - 444s 494ms/step - loss: 1.2774 - accuracy: 0.5131 - val_loss: 1.2629 - val_accuracy: 0.5076\n",
            "Epoch 5/5\n",
            "898/898 [==============================] - 444s 494ms/step - loss: 1.2301 - accuracy: 0.5320 - val_loss: 1.2374 - val_accuracy: 0.5331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.save('model_2.h5')"
      ],
      "metadata": {
        "id": "p2u9xvGA6PSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_2 = tf.keras.models.load_model(\"model_2.h5\")"
      ],
      "metadata": {
        "id": "QduAQUmhEGtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_2.evaluate(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_aMlJYTENvh",
        "outputId": "51796adb-bc66-4d02-a686-3cf546099607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "225/225 [==============================] - 29s 127ms/step - loss: 1.2145 - accuracy: 0.5382\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2144746780395508, 0.53817218542099]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.save(\"model_2_SavedModel_format\")"
      ],
      "metadata": {
        "id": "TyJoqp8WEU5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "loaded_model_2_SavedModel = tf.keras.models.load_model(\"model_2_SavedModel_format\")"
      ],
      "metadata": {
        "id": "P_5oH1ioElwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_2_SavedModel.evaluate(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foK5UDssErz0",
        "outputId": "fd16c375-73fd-475f-bc94-175ac11c885e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "225/225 [==============================] - 31s 137ms/step - loss: 1.2145 - accuracy: 0.5382\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2144743204116821, 0.53817218542099]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_json = model_2.to_json()\n",
        "with open(\"model_2.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# save trained model weight in .h5 file\n",
        "model_2.save_weights('model_2.h5')"
      ],
      "metadata": {
        "id": "9KKwhaSMcmxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.io.gfile.GFile(name='model_2.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHm7DsS3euwb",
        "outputId": "70b746ef-86c0-45bd-f90e-9dab477e0e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.platform.gfile.GFile at 0x7ff7ff682950>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import model_from_json\n",
        "\n",
        "\n",
        "emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprise\"}\n",
        "\n",
        "# load json and create model\n",
        "json_file = open('model_2.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "emotion_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "emotion_model.load_weights(\"model_2.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "# start the webcam feed\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "    # Find haar cascade to draw bounding box around face\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "    face_detector = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # detect faces available on camera\n",
        "    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    # take each face available on the camera and Preprocess it\n",
        "    for (x, y, w, h) in num_faces:\n",
        "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
        "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
        "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
        "\n",
        "        # predict the emotions\n",
        "        emotion_prediction = emotion_model.predict(cropped_img)\n",
        "        maxindex = int(np.argmax(emotion_prediction))\n",
        "        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "    cv2.imshow('Emotion Detection', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weEog2DCoYkW",
        "outputId": "4c538b06-c35d-4c72-f962-4482d4ebaa74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "take_photo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "o_LOgqeGq3DF",
        "outputId": "3a3c3167-c702-4b41-b8bf-fa0f5b9270fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'photo.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.save_weights('model_2.h5')\n",
        "model_2.save('model_2.h5')"
      ],
      "metadata": {
        "id": "tetldJGEdVFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "\n",
        "    # start the webcam feed\n",
        "cap = cv2.VideoCapture('photo.jpg')\n",
        "while True:\n",
        "        # Find haar cascade to draw bounding box around face\n",
        "   ret, frame = cap.read()\n",
        "   if not ret:\n",
        "     break\n",
        "   facecasc = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
        "   gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "   faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "   for (x, y, w, h) in faces:\n",
        "      cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "      roi_gray = gray[y:y + h, x:x + w]\n",
        "      cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
        "      prediction = model.predict(cropped_img)\n",
        "      maxindex = int(np.argmax(prediction))\n",
        "      cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "      cv2.imshow('Video', cv2.resize(frame,(1600,960),interpolation = cv2.INTER_CUBIC))\n",
        "      if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "         break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "Otlm_v2CtvSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\") #Note the change\n",
        "\n",
        "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "\n",
        "img = cv2.imread(\"photo.jpg\")\n",
        "img.reshape((48,48,3))\n",
        "gray_img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "faces=face_cascade.detectMultiScale(gray_img, scaleFactor=1.05,minNeighbors=5)\n",
        "\n",
        "for x, y, w, h in faces:\n",
        "    cv2.rectangle(img, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "    prediction = model_2.predict(img)\n",
        "    maxindex = int(np.argmax(prediction))\n",
        "    cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "resized=cv2.resize(img,(int(img.shape[1]/3), int(img.shape[0]/3))) \n",
        "\n",
        "cv2_imshow(resized)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "c_cskGmGuK7Q",
        "outputId": "18a7bc23-8a07-4479-f9bb-6c420194d5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-453d7262ea44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"photo.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mgray_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 921600 into shape (48,48,3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VqUmpeYOvKFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}